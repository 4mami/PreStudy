{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreStudy.ipynb",
      "provenance": [],
      "mount_file_id": "1kpTCb3bzRsS4zn8uFB0LR7pF2jiaYDp8",
      "authorship_tag": "ABX9TyO/9CqxzkGvgl4JP6dh4QJU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4mami/PreStudy/blob/main/PreStudy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRXjXwwpobMZ"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install fugashi\n",
        "!pip install ipadic"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertJapaneseTokenizer"
      ],
      "metadata": {
        "id": "YWOCC35YsoiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"cl-tohoku/bert-base-japanese-whole-word-masking\"\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "id": "Mj-rQoOQwzwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassifyTwitterUserToMultiLabel(torch.nn.Module):\n",
        "    def __init__(self, embedding_dim, num_labels):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(len(tokenizer.vocab), embedding_dim, padding_idx=0)\n",
        "        self.linear = torch.nn.Linear(embedding_dim, num_labels)\n",
        "\n",
        "    def forward(self, input_tensor):\n",
        "        tensor = self.embedding(input_tensor) # (1, 990, 120, Edim)\n",
        "        tensor = tensor.mean(2)\n",
        "        tensor = tensor.mean(1)\n",
        "\n",
        "        return self.linear(tensor)"
      ],
      "metadata": {
        "id": "0Hgu27HRTLF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jsonファイルから読み込んだデータを基に、(ラベルを表すリスト, 単語ID化したツイートのリスト)のタプルのリストdata_listを作成する\n",
        "import json\n",
        "data = json.load(open('drive/MyDrive/Data/TrainingData.json'))\n",
        "\n",
        "data_list = []\n",
        "\n",
        "for datum in data:\n",
        "    label = datum[\"label\"]\n",
        "    tweets = datum[\"tweets\"]\n",
        "    tweets_ids = tokenizer(tweets, padding=\"longest\")[\"input_ids\"]\n",
        "\n",
        "    data_list.append((label, tweets_ids))\n",
        "\n",
        "print(data_list[0])"
      ],
      "metadata": {
        "id": "wLbj3ma3olFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "5Kqu9i9V0SQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_data_list):\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "\n",
        "    for data in train_data_list:\n",
        "        input_tensor = torch.tensor(data[1])\n",
        "        input_tensor = input_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        label_tensor = torch.tensor(data[0]).unsqueeze(0).to(device)\n",
        "        output_tensor = model(input_tensor)\n",
        "\n",
        "        loss = criterion(output_tensor, label_tensor.float())\n",
        "        train_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        label_predicted = (output_tensor > 0).int()\n",
        "        num_correct = (label_predicted == label_tensor).int().sum().item()\n",
        "        acc = num_correct / CAT_SIZE\n",
        "        train_acc += acc\n",
        "\n",
        "    scheduler.step()\n",
        "    return train_loss / len(train_data_list), train_acc / len(train_data_list)"
      ],
      "metadata": {
        "id": "M7EnPUTEt8jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(valid_data_list):\n",
        "    valid_loss = 0\n",
        "    valid_acc = 0\n",
        "    divided_acc = [0] * 6\n",
        "\n",
        "    for data in valid_data_list:\n",
        "        input_tensor = torch.tensor(data[1])\n",
        "        input_tensor = input_tensor.unsqueeze(0).to(device)\n",
        "        label_tensor = torch.tensor(data[0]).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output_tensor = model(input_tensor)\n",
        "            loss = criterion(output_tensor, label_tensor.float())\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "            label_predicted = (output_tensor > 0).int()\n",
        "            num_correct = (label_predicted == label_tensor).int().sum().item()\n",
        "            acc = num_correct / CAT_SIZE\n",
        "            valid_acc += acc\n",
        "\n",
        "            for i in range(6):\n",
        "                if ((label_predicted[0][i] == label_tensor[0][i]).item()):\n",
        "                    divided_acc[i] += 1\n",
        "\n",
        "    return valid_loss / len(valid_data_list), valid_acc / len(valid_data_list), divided_acc"
      ],
      "metadata": {
        "id": "G8m3T0iRuiCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import random\n",
        "NUM_EPOCH = 80\n",
        "NUM_FOLD = 5\n",
        "CAT_SIZE = 6\n",
        "EMBEDDED_DIM = 768\n",
        "criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
        "mean_train_acc = 0.0\n",
        "mean_valid_acc = 0.0\n",
        "\n",
        "start_time = time.time()\n",
        "for fold in range(NUM_FOLD):\n",
        "    # モデルの性能を見る以上、各foldごとにモデルの学習成果をリセットしないといけないから、毎foldでmodel,optimizer,schedulerを代入し直す\n",
        "    model = ClassifyTwitterUserToMultiLabel(EMBEDDED_DIM, CAT_SIZE)\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
        "\n",
        "    valid_data_list = data_list[fold*20:fold*20+20]\n",
        "    train_data_list = [] + data_list[:fold*20]\n",
        "    train_data_list += data_list[fold*20+20:]\n",
        "\n",
        "    for epoch in range(NUM_EPOCH):\n",
        "        random.shuffle(train_data_list)\n",
        "        train_loss, train_acc = train(train_data_list)\n",
        "        valid_loss, valid_acc, divided_acc = validation(valid_data_list)\n",
        "\n",
        "        secs = int(time.time() - start_time)\n",
        "        mins = secs / 60\n",
        "        secs = secs % 60\n",
        "\n",
        "        print('Fold: %d' %(fold + 1), 'Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
        "        print(f'\\t\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)\\t|\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)', \"各ラベルごとの正解数：\", divided_acc)\n",
        "\n",
        "    mean_train_acc += train_acc / NUM_FOLD\n",
        "    mean_valid_acc += valid_acc / NUM_FOLD\n",
        "    print(\"-----------------------------------------------------------------------------------------------------------------------\")\n",
        "\n",
        "print(f\"Mean Train Acc: {mean_train_acc * 100:.1f}%\")\n",
        "print(f\"Mean Valid Acc: {mean_valid_acc * 100:.1f}%\")"
      ],
      "metadata": {
        "id": "OUYoxUtuz6q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データ内の各ラベルの合計数を出力\n",
        "ind_0 = 0\n",
        "ind_1 = 0\n",
        "ind_2 = 0\n",
        "ind_3 = 0\n",
        "ind_4 = 0\n",
        "ind_5 = 0\n",
        "\n",
        "for i, data in enumerate(data_list):\n",
        "    ind_0 += data[0][0]\n",
        "    ind_1 += data[0][1]\n",
        "    ind_2 += data[0][2]\n",
        "    ind_3 += data[0][3]\n",
        "    ind_4 += data[0][4]\n",
        "    ind_5 += data[0][5]\n",
        "\n",
        "    if ((i+1) % 20 == 0):\n",
        "        print(i+1, \"個までの各ラベル合計数：\", ind_0, \",\", ind_1, \",\", ind_2, \",\", ind_3, \",\", ind_4, \",\", ind_5)\n",
        "        ind_0 = 0\n",
        "        ind_1 = 0\n",
        "        ind_2 = 0\n",
        "        ind_3 = 0\n",
        "        ind_4 = 0\n",
        "        ind_5 = 0\n"
      ],
      "metadata": {
        "id": "tC518dGDL-nF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}